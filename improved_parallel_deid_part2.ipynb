{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Parallel De-identification with Ollama (Part 2)\n",
    "\n",
    "This notebook is a continuation of `improved_parallel_deid.ipynb`. It contains the remaining implementation of the main processing function and execution code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions from the first notebook\n",
    "%run improved_parallel_deid.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Main Processing Function\n",
    "\n",
    "This completes the implementation of the `process_large_csv` function from the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_csv_complete(input_path, output_prefix, columns_to_clean):\n",
    "    \"\"\"\n",
    "    Reads a potentially large CSV, splits it into batches, de-identifies the \n",
    "    specified columns, and saves new numbered CSV files for each batch.\n",
    "    Uses the specified implementation approach for concurrency control.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_path) or input_path == \"/path/to/your/file.csv\":\n",
    "        print(\"ERROR: Input file not found or path not set.\")\n",
    "        print(f\"Please update the 'INPUT_CSV_PATH' variable in the Configuration section.\")\n",
    "        return\n",
    "    \n",
    "    # Get the directory of the input file to save output files in the same location\n",
    "    output_dir = os.path.dirname(os.path.abspath(input_path))\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading and preparing CSV from '{input_path}'...\")\n",
    "        df_iterator = pd.read_csv(input_path, chunksize=MAX_ROWS_PER_BATCH, on_bad_lines='warn')\n",
    "        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            total_rows = sum(1 for row in f) - 1 # -1 for header\n",
    "        num_batches = math.ceil(total_rows / MAX_ROWS_PER_BATCH)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading input file: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total rows: {total_rows}. This will be processed in {num_batches} batch(es).\")\n",
    "    print(f\"Output files will be saved in: {output_dir}\")\n",
    "    print(f\"Each text will undergo {DEIDENTIFICATION_PASSES} pass(es) through the LLM.\")\n",
    "    print(f\"Using implementation approach: {IMPLEMENTATION_APPROACH}\")\n",
    "    print(f\"Maximum concurrent requests to Ollama: {MAX_CONCURRENT_REQUESTS}\")\n",
    "    \n",
    "    # Track processed batches for summary\n",
    "    processed_batches = []\n",
    "    skipped_batches = []\n",
    "\n",
    "    for i, batch_df in enumerate(df_iterator):\n",
    "        batch_num = i + 1\n",
    "        output_path = os.path.join(output_dir, f\"{output_prefix}_part_{batch_num}.csv\")\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"\\nOutput file '{output_path}' already exists. Skipping Batch {batch_num}.\")\n",
    "            skipped_batches.append(batch_num)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Processing Batch {batch_num}/{num_batches} ---\")\n",
    "        \n",
    "        for column_name in columns_to_clean:\n",
    "            if column_name not in batch_df.columns:\n",
    "                print(f\"  - WARNING: Column '{column_name}' not found in this batch. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  - De-identifying column: '{column_name}' using {IMPLEMENTATION_APPROACH} approach\")\n",
    "            \n",
    "            total_in_batch = len(batch_df)\n",
    "            \n",
    "            # Prepare data for processing: (row_index, text_content)\n",
    "            row_data = [(idx, row[column_name]) for idx, row in batch_df.iterrows()]\n",
    "            \n",
    "            # Choose the appropriate processing method based on the implementation approach\n",
    "            if IMPLEMENTATION_APPROACH == \"queue\":\n",
    "                processed_results = process_with_queue(row_data, total_in_batch)\n",
    "            elif IMPLEMENTATION_APPROACH == \"process_pool\":\n",
    "                processed_results = process_with_process_pool(row_data, total_in_batch)\n",
    "            else:  # Default to ThreadPoolExecutor with semaphore or rate limiting\n",
    "                # Initialize results dictionary to maintain order\n",
    "                processed_results = {}\n",
    "                completed_count = 0\n",
    "                \n",
    "                # Use ThreadPoolExecutor for parallel processing\n",
    "                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                    # Submit all tasks\n",
    "                    future_to_index = {executor.submit(process_row_parallel, data): data[0] for data in row_data}\n",
    "                    \n",
    "                    # Process completed tasks as they finish\n",
    "                    for future in as_completed(future_to_index):\n",
    "                        original_index, processed_text = future.result()\n",
    "                        processed_results[original_index] = processed_text\n",
    "                        completed_count += 1\n",
    "                        \n",
    "                        # Print progress\n",
    "                        print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "            \n",
    "            print()  # Newline after the progress bar for a column is complete\n",
    "            \n",
    "            # Reconstruct the processed data in the original order\n",
    "            processed_data = [processed_results[idx] for idx, _ in batch_df.iterrows()]\n",
    "            \n",
    "            # Update the dataframe with processed data\n",
    "            if REPLACE_ORIGINAL_COLUMN:\n",
    "                batch_df[column_name] = processed_data\n",
    "            else:\n",
    "                batch_df[f\"{column_name}_deidentified\"] = processed_data\n",
    "        \n",
    "        # Save the processed batch to CSV\n",
    "        try:\n",
    "            batch_df.to_csv(output_path, index=False)\n",
    "            print(f\"  - Saved batch to: '{output_path}'\")\n",
    "            processed_batches.append(batch_num)\n",
    "        except Exception as e:\n",
    "            print(f\"  - ERROR: Could not save batch {batch_num}: {e}\")\n",
    "    \n",
    "    print(f\"\\n--- Processing Complete ---\")\n",
    "    print(f\"All batches have been processed and saved with prefix '{output_prefix}'.\")\n",
    "    \n",
    "    # Print summary of processed and skipped batches\n",
    "    if processed_batches:\n",
    "        print(f\"\\nProcessed batches: {', '.join(map(str, processed_batches))}\")\n",
    "    if skipped_batches:\n",
    "        print(f\"Skipped batches (already existed): {', '.join(map(str, skipped_batches))}\")\n",
    "    \n",
    "    # Print the location of the output files\n",
    "    print(f\"\\nOutput files are located in: {output_dir}\")\n",
    "    print(f\"File naming pattern: {output_prefix}_part_X.csv where X is the batch number\")\n",
    "    \n",
    "    # List the output files that exist\n",
    "    existing_output_files = [f for f in os.listdir(output_dir) if f.startswith(output_prefix) and f.endswith('.csv')]\n",
    "    if existing_output_files:\n",
    "        print(f\"\\nFound {len(existing_output_files)} output files:\")\n",
    "        for file in sorted(existing_output_files):\n",
    "            file_path = os.path.join(output_dir, file)\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
    "            print(f\"  - {file} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"\\nNo output files found with prefix '{output_prefix}' in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Process\n",
    "\n",
    "Execute the main function. This will start the process using the file and columns you specified in the Configuration section. If you run this cell a second time, it will find the generated output files and skip the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs the main function with the settings you provided in the first notebook.\n",
    "process_large_csv_complete(\n",
    "    input_path=INPUT_CSV_PATH,\n",
    "    output_prefix=OUTPUT_PREFIX, \n",
    "    columns_to_clean=COLUMNS_TO_CLEAN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting\n",
    "\n",
    "If you're still experiencing issues with Ollama and parallel processing, here are some additional approaches to try:\n",
    "\n",
    "1. **Further reduce concurrency**: Try setting `MAX_CONCURRENT_REQUESTS` to 1 or 2\n",
    "2. **Increase delay between requests**: Modify the rate limiter to add more delay\n",
    "3. **Use a different model**: Some models may handle concurrent requests better than others\n",
    "4. **Run Ollama with more resources**: If possible, allocate more CPU/memory to Ollama\n",
    "5. **Use a different implementation approach**: Try each of the approaches (semaphore, rate_limit, queue, process_pool) to see which works best for your system\n",
    "\n",
    "You can also try running Ollama with the `--parallel` flag if available in your version, which may improve handling of concurrent requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of modifying settings for troubleshooting\n",
    "def troubleshoot_with_minimal_concurrency():\n",
    "    global MAX_CONCURRENT_REQUESTS, RATE_LIMIT_CALLS, RATE_LIMIT_PERIOD\n",
    "    \n",
    "    # Save original values\n",
    "    original_max_concurrent = MAX_CONCURRENT_REQUESTS\n",
    "    original_rate_limit_calls = RATE_LIMIT_CALLS\n",
    "    original_rate_limit_period = RATE_LIMIT_PERIOD\n",
    "    \n",
    "    # Apply minimal concurrency settings\n",
    "    MAX_CONCURRENT_REQUESTS = 1\n",
    "    RATE_LIMIT_CALLS = 1\n",
    "    RATE_LIMIT_PERIOD = 2  # 1 call every 2 seconds\n",
    "    \n",
    "    # Recreate the semaphore and rate limiter with new settings\n",
    "    global api_semaphore, rate_limiter\n",
    "    api_semaphore = threading.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "    rate_limiter = RateLimiter(RATE_LIMIT_CALLS, RATE_LIMIT_PERIOD)\n",
    "    \n",
    "    print(f\"Applied minimal concurrency settings:\")\n",
    "    print(f\"  - MAX_CONCURRENT_REQUESTS: {MAX_CONCURRENT_REQUESTS} (was {original_max_concurrent})\")\n",
    "    print(f\"  - RATE_LIMIT_CALLS: {RATE_LIMIT_CALLS} (was {original_rate_limit_calls})\")\n",
    "    print(f\"  - RATE_LIMIT_PERIOD: {RATE_LIMIT_PERIOD} (was {original_rate_limit_period})\")\n",
    "    print(f\"\\nNow you can run the process_large_csv_complete function again with these settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you're still experiencing issues\n",
    "# troubleshoot_with_minimal_concurrency()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
