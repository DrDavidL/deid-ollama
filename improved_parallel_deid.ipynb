{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel De-identification with Ollama\n",
    "\n",
    "This notebook provides a guide to de-identifying data in parallel using Ollama. It offers several approaches to manage concurrent requests to the Ollama API, ensuring robust and efficient processing.\n",
    "\n",
    "**How to Select an Implementation Approach:**\n",
    "You can choose the implementation approach by setting the `IMPLEMENTATION_APPROACH` variable in the **Configuration** section below. The default is `'semaphore'`, which is a reliable and performant option for most use cases.\n",
    "\n",
    "**Troubleshooting Guidance:**\n",
    "If you experience issues such as timeouts or connection errors, you can adjust the settings in the **Configuration** section. Here are some specific recommendations:\n",
    "\n",
    "1.  **Reduce `MAX_CONCURRENT_REQUESTS`**: Start by lowering this value to `2` or `1`. This will significantly reduce the load on the Ollama server.\n",
    "2.  **Try the `queue` approach**: If reducing concurrent requests doesn't solve the issue, change the `IMPLEMENTATION_APPROACH` to `'queue'`. This can provide more stability in some environments.\n",
    "3.  **Increase `MAX_RETRIES`**: If you are still seeing occasional errors, you can increase the `MAX_RETRIES` value to `5` or `6` to give the script more chances to succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import threading\n",
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Before running the notebook, please specify the required variables in the following cell. This includes the path to your input CSV file, the columns to de-identify, and other processing settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REQUIRED: Specify the path to your CSV file ---\n",
    "INPUT_CSV_PATH = \"tester.csv\" \n",
    "\n",
    "# --- REQUIRED: List the names of the columns you want to de-identify ---\n",
    "COLUMNS_TO_CLEAN = [\"patient_id\", \"first_name\", \"last_name\", \"dob\", \"phone_number\",\"note_text\"] \n",
    "\n",
    "# --- OPTIONAL: Advanced Settings ---\n",
    "REPLACE_ORIGINAL_COLUMN = True      # True: Replaces original column(s). False: Adds new de-identified column(s).\n",
    "OUTPUT_PREFIX = \"deidentified_output_post_LLM\" # The prefix for the output files\n",
    "MAX_ROWS_PER_BATCH = 100         # The max number of rows to process in a single batch file.\n",
    "\n",
    "# --- Concurrency Settings ---\n",
    "MAX_WORKERS = 5                   # Reduced from 10 to avoid overwhelming Ollama\n",
    "MAX_CONCURRENT_REQUESTS = 3       # Maximum number of concurrent requests to Ollama\n",
    "USE_RATE_LIMITING = True          # Enable rate limiting for API calls\n",
    "RATE_LIMIT_CALLS = 3              # Maximum calls per time period\n",
    "RATE_LIMIT_PERIOD = 1             # Time period in seconds\n",
    "\n",
    "# --- Processing Settings ---\n",
    "MAX_RETRIES = 4                    # Number of times to retry processing a row before marking it as \"unable to deidentify\"\n",
    "DEIDENTIFICATION_PASSES = 2        # Number of passes through the LLM to catch any PHI missed in the first pass\n",
    "IMPLEMENTATION_APPROACH = \"semaphore\"  # Options: \"semaphore\", \"rate_limit\", \"queue\", \"process_pool\"\n",
    "\n",
    "# --- Ollama Settings ---\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"gemma3:4b\"             # Or any other model you have available\n",
    "MAX_CHUNK_SIZE = 5000               # Max characters for a single note before it's split for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Functions for Ollama API Interaction\n",
    "\n",
    "These functions handle the communication with the Ollama API. They include different strategies for managing concurrency to prevent overwhelming the API with too many requests at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a semaphore to limit concurrent API calls\n",
    "api_semaphore = threading.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# Create a rate limiter\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls, period):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = []\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            # Remove calls older than the period\n",
    "            self.calls = [t for t in self.calls if now - t < self.period]\n",
    "            \n",
    "            # If we've reached the maximum calls, wait until we can make another\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_time = self.period - (now - self.calls[0])\n",
    "                if sleep_time > 0:\n",
    "                    time.sleep(sleep_time)\n",
    "                    \n",
    "            # Add the current call time\n",
    "            self.calls.append(time.time())\n",
    "            \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "\n",
    "rate_limiter = RateLimiter(RATE_LIMIT_CALLS, RATE_LIMIT_PERIOD)\n",
    "\n",
    "# Queue for controlled processing\n",
    "request_queue = queue.Queue()\n",
    "response_queue = queue.Queue()\n",
    "\n",
    "def call_ollama_api_with_semaphore(text_chunk, row_index=None, retry_count=0):\n",
    "    \"\"\"\n",
    "    Sends a text chunk to the Ollama API with semaphore-based concurrency control.\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "    \n",
    "    prompt = create_prompt(text_chunk)\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    # Use the semaphore to limit concurrent requests\n",
    "    with api_semaphore:\n",
    "        try:\n",
    "            # Add jitter to avoid thundering herd problem\n",
    "            time.sleep(random.uniform(0.1, 0.5))\n",
    "            response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "            return response_data.get('response', '').strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retry_count += 1\n",
    "            if row_index is not None:\n",
    "                print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            if retry_count < MAX_RETRIES:\n",
    "                backoff_time = min(2 ** retry_count + random.uniform(0, 1), 10)\n",
    "                time.sleep(backoff_time)\n",
    "                return call_ollama_api_with_semaphore(text_chunk, row_index, retry_count)\n",
    "            else:\n",
    "                return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "\n",
    "def call_ollama_api_with_rate_limit(text_chunk, row_index=None, retry_count=0):\n",
    "    \"\"\"\n",
    "    Sends a text chunk to the Ollama API with rate limiting.\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "    \n",
    "    prompt = create_prompt(text_chunk)\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    # Use rate limiting\n",
    "    with rate_limiter:\n",
    "        try:\n",
    "            response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "            return response_data.get('response', '').strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retry_count += 1\n",
    "            if row_index is not None:\n",
    "                print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            if retry_count < MAX_RETRIES:\n",
    "                backoff_time = min(2 ** retry_count + random.uniform(0, 1), 10)\n",
    "                time.sleep(backoff_time)\n",
    "                return call_ollama_api_with_rate_limit(text_chunk, row_index, retry_count)\n",
    "            else:\n",
    "                return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "\n",
    "def queue_worker():\n",
    "    \"\"\"\n",
    "    Worker function for queue-based processing.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # Get a task from the queue\n",
    "            task = request_queue.get(timeout=1)\n",
    "            if task is None:  # Sentinel value to stop the worker\n",
    "                request_queue.task_done()\n",
    "                break\n",
    "                \n",
    "            text_chunk, row_index, retry_count = task\n",
    "            \n",
    "            # Process the task\n",
    "            try:\n",
    "                prompt = create_prompt(text_chunk)\n",
    "                payload = {\n",
    "                    \"model\": MODEL_NAME,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False\n",
    "                }\n",
    "                \n",
    "                # Add jitter to avoid thundering herd problem\n",
    "                time.sleep(random.uniform(0.1, 0.5))\n",
    "                response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "                response.raise_for_status()\n",
    "                response_data = response.json()\n",
    "                result = response_data.get('response', '').strip()\n",
    "                \n",
    "                # Put the result in the response queue\n",
    "                response_queue.put((row_index, result))\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                retry_count += 1\n",
    "                if row_index is not None:\n",
    "                    print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "                \n",
    "                # Retry if we haven't reached the maximum retries\n",
    "                if retry_count < MAX_RETRIES:\n",
    "                    # Exponential backoff with jitter\n",
    "                    backoff_time = min(2 ** retry_count + random.uniform(0, 1), 10)\n",
    "                    time.sleep(backoff_time)\n",
    "                    request_queue.put((text_chunk, row_index, retry_count))\n",
    "                else:\n",
    "                    response_queue.put((row_index, \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"))\n",
    "            \n",
    "            # Mark the task as done\n",
    "            request_queue.task_done()\n",
    "                \n",
    "        except queue.Empty:\n",
    "            # If the queue is empty, wait a bit and try again\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in queue worker: {e}\")\n",
    "            request_queue.task_done()\n",
    "\n",
    "def create_prompt(text_chunk):\n",
    "    \"\"\"\n",
    "    Creates the prompt for the Ollama API with enhanced PHI detection.\n",
    "    \"\"\"\n",
    "    return f\"\"\"[SYSTEM]\n",
    "    You are an automated de-identification system. Your sole function is to process the text provided and return a clean version with all Protected Health Information (PHI) replaced by the appropriate category label. You are precise and do not deviate from your instructions.\n",
    "\n",
    "    **Instructions:**\n",
    "\n",
    "    1.  **Identify and Replace:** Your task is to find all instances of the following PHI categories in the user-provided text and replace them with their corresponding labels:\n",
    "        * Names of individuals: `[PERSON]`\n",
    "        * All dates (including full dates, partial dates, and days of the week): `[DATE]`\n",
    "        * Geographical locations (cities, states, addresses, etc.): `[LOCATION]`\n",
    "        * Phone and fax numbers: `[PHONE]`\n",
    "        * Email addresses: `[EMAIL]`\n",
    "        * Any identification numbers including patient IDs, medical record numbers, account numbers, social security numbers, etc.: `[ID_NUMBER]`\n",
    "        * Other categories such as medical conditions, medications, and health-related information should remain unchanged unless they contain PHI.\n",
    "\n",
    "    2.  **Output Format:**\n",
    "        * The output MUST be only the modified text.\n",
    "        * Do NOT include any introductory phrases, explanations, or apologies.\n",
    "        * The response should not contain any of the original PHI.\n",
    "        * If the input is ONLY a number that could be an ID, replace it with [ID_NUMBER].\n",
    "\n",
    "    **Examples:**\n",
    "\n",
    "    * **Input:** \"John Smith visited the clinic on January 5, 2024. His MRN is 12345. He has HTN and takes lisinopril.\"\n",
    "    * **Output:** \"[PERSON] visited the clinic on [DATE]. His MRN is [ID_NUMBER]. He has HTN and takes lisinopril.\"\n",
    "\n",
    "    * **Input:** \"101\"\n",
    "    * **Output:** \"[ID_NUMBER]\"\n",
    "\n",
    "    * **Input:** \"Patient ID: 12345\"\n",
    "    * **Output:** \"Patient ID: [ID_NUMBER]\"\n",
    "\n",
    "    * **Input:** \"Patient can be reached at (555) 123-4567 or jane.doe@email.com.\"\n",
    "    * **Output:** \"Patient can be reached at [PHONE] or [EMAIL].\"\n",
    "\n",
    "    **Text for De-identification:**\n",
    "\n",
    "    ---\n",
    "    {text_chunk}\n",
    "    ---\n",
    "    \"\"\"\n",
    "\n",
    "def deidentify_text(full_text, row_index=None, implementation=IMPLEMENTATION_APPROACH):\n",
    "    \"\"\"\n",
    "    Manages the de-identification of a full text string, handling chunking if necessary.\n",
    "    Performs multiple passes through the LLM based on DEIDENTIFICATION_PASSES setting.\n",
    "    Uses the specified implementation approach for API calls.\n",
    "    \"\"\"\n",
    "    if not isinstance(full_text, str) or not full_text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Select the appropriate API call function based on the implementation approach\n",
    "    if implementation == \"semaphore\":\n",
    "        api_call_func = call_ollama_api_with_semaphore\n",
    "    elif implementation == \"rate_limit\":\n",
    "        api_call_func = call_ollama_api_with_rate_limit\n",
    "    else:\n",
    "        # Default to semaphore-based approach\n",
    "        api_call_func = call_ollama_api_with_semaphore\n",
    "    \n",
    "    # Process the text through multiple passes to catch any missed PHI\n",
    "    processed_text = full_text\n",
    "    \n",
    "    for pass_num in range(DEIDENTIFICATION_PASSES):\n",
    "        if pass_num > 0 and row_index is not None:\n",
    "            print(f\"\\r    - Row {row_index}: Pass {pass_num+1}/{DEIDENTIFICATION_PASSES}\", end=\"\")\n",
    "            \n",
    "        if len(processed_text) <= MAX_CHUNK_SIZE:\n",
    "            processed_text = api_call_func(processed_text, row_index)\n",
    "        else:\n",
    "            chunks = [processed_text[i:i + MAX_CHUNK_SIZE] for i in range(0, len(processed_text), MAX_CHUNK_SIZE)]\n",
    "            processed_chunks = []\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                processed_chunk = api_call_func(chunk, row_index)\n",
    "                processed_chunks.append(processed_chunk)\n",
    "                \n",
    "            processed_text = \"\".join(processed_chunks)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def process_row_parallel(args):\n",
    "    \"\"\"\n",
    "    Helper function for parallel processing of individual rows.\n",
    "    Returns tuple of (original_index, processed_text) to maintain order.\n",
    "    \"\"\"\n",
    "    row_index, original_text = args\n",
    "    processed_text = deidentify_text(original_text, row_index, IMPLEMENTATION_APPROACH)\n",
    "    return (row_index, processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing Implementations\n",
    "\n",
    "This section contains different implementations for processing the data. You can choose the one that best fits your needs by setting the `IMPLEMENTATION_APPROACH` variable in the configuration section.\n",
    "\n",
    "### Semaphore-based Approach\n",
    "A semaphore is a synchronization primitive that controls access to a shared resource. In this context, it limits the number of concurrent API calls to Ollama. This is a simple and effective way to prevent overloading the API.\n",
    "\n",
    "### Rate Limiting Approach\n",
    "Rate limiting controls the number of requests sent to the API within a specific time period. This approach is useful for APIs that have a strict rate limit policy.\n",
    "\n",
    "### Queue-based Approach\n",
    "A queue-based approach provides a more controlled way of processing the data. Requests are added to a queue and processed by a pool of workers. This ensures that the number of concurrent requests never exceeds the specified limit and provides a buffer for incoming requests.\n",
    "\n",
    "### Process Pool Approach\n",
    "This approach uses a pool of processes to execute the de-identification tasks in parallel. It can be more efficient for CPU-bound tasks, as it leverages multiple CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_queue(row_data, total_in_batch):\n",
    "    \"\"\"\n",
    "    Process rows using a queue-based approach for better control over concurrency.\n",
    "    \"\"\"\n",
    "    # Clear the queues\n",
    "    while not request_queue.empty():\n",
    "        try:\n",
    "            request_queue.get_nowait()\n",
    "            request_queue.task_done()\n",
    "        except queue.Empty:\n",
    "            break\n",
    "            \n",
    "    while not response_queue.empty():\n",
    "        try:\n",
    "            response_queue.get_nowait()\n",
    "        except queue.Empty:\n",
    "            break\n",
    "    \n",
    "    # Start worker threads\n",
    "    workers = []\n",
    "    for _ in range(MAX_CONCURRENT_REQUESTS):\n",
    "        worker = threading.Thread(target=queue_worker)\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "        workers.append(worker)\n",
    "    \n",
    "    # Add tasks to the queue\n",
    "    for row_index, original_text in row_data:\n",
    "        request_queue.put((original_text, row_index, 0))\n",
    "    \n",
    "    # Wait for all tasks to be processed\n",
    "    processed_results = {}\n",
    "    completed_count = 0\n",
    "    \n",
    "    while completed_count < len(row_data):\n",
    "        try:\n",
    "            row_index, processed_text = response_queue.get(timeout=1)\n",
    "            processed_results[row_index] = processed_text\n",
    "            completed_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "            \n",
    "        except queue.Empty:\n",
    "            # If the queue is empty, wait a bit and try again\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in queue worker: {e}\")\n",
    "            request_queue.task_done()\n",
    "\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Processing Function\n",
    "\n",
    "This is the main function that orchestrates the de-identification process. It reads the input CSV, splits it into batches, and processes each batch using the selected implementation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_process_pool(row_data, total_in_batch):\n",
    "    \"\"\"\n",
    "    Process rows using ProcessPoolExecutor for potentially better performance on CPU-bound tasks.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary to maintain order\n",
    "    processed_results = {}\n",
    "    completed_count = 0\n",
    "    \n",
    "    # Define a worker function that can be pickled\n",
    "    def worker_func(row_index, text):\n",
    "        return row_index, deidentify_text(text, row_index, \"semaphore\")\n",
    "    \n",
    "    # Use ProcessPoolExecutor for parallel processing\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_index = {executor.submit(worker_func, idx, text): idx for idx, text in row_data}\n",
    "        \n",
    "        # Process completed tasks as they finish\n",
    "        for future in as_completed(future_to_index):\n",
    "            original_index, processed_text = future.result()\n",
    "            processed_results[original_index] = processed_text\n",
    "            completed_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "    \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Process\n",
    "\n",
    "Execute the following cell to start the de-identification process. The script will use the settings you provided in the Configuration section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_csv_complete(input_path, output_prefix, columns_to_clean):\n",
    "    \"\"\"\n",
    "    Reads a potentially large CSV, splits it into batches, de-identifies the \n",
    "    specified columns, and saves new numbered CSV files for each batch.\n",
    "    Uses the specified implementation approach for concurrency control.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_path) or input_path == \"/path/to/your/file.csv\":\n",
    "        print(\"ERROR: Input file not found or path not set.\")\n",
    "        print(f\"Please update the 'INPUT_CSV_PATH' variable in the Configuration section.\")\n",
    "        return\n",
    "    \n",
    "    # Get the directory of the input file to save output files in the same location\n",
    "    output_dir = os.path.dirname(os.path.abspath(input_path))\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading and preparing CSV from '{input_path}'...\")\n",
    "        df_iterator = pd.read_csv(input_path, chunksize=MAX_ROWS_PER_BATCH, on_bad_lines='warn')\n",
    "        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            total_rows = sum(1 for row in f) - 1 # -1 for header\n",
    "        num_batches = math.ceil(total_rows / MAX_ROWS_PER_BATCH)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading input file: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total rows: {total_rows}. This will be processed in {num_batches} batch(es).\")\n",
    "    print(f\"Output files will be saved in: {output_dir}\")\n",
    "    print(f\"Each text will undergo {DEIDENTIFICATION_PASSES} pass(es) through the LLM.\")\n",
    "    print(f\"Using implementation approach: {IMPLEMENTATION_APPROACH}\")\n",
    "    print(f\"Maximum concurrent requests to Ollama: {MAX_CONCURRENT_REQUESTS}\")\n",
    "    \n",
    "    # Track processed batches for summary\n",
    "    processed_batches = []\n",
    "    skipped_batches = []\n",
    "\n",
    "    for i, batch_df in enumerate(df_iterator):\n",
    "        batch_num = i + 1\n",
    "        output_path = os.path.join(output_dir, f\"{output_prefix}_part_{batch_num}.csv\")\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"\\nOutput file '{output_path}' already exists. Skipping Batch {batch_num}.\")\n",
    "            skipped_batches.append(batch_num)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Processing Batch {batch_num}/{num_batches} ---\")\n",
    "        \n",
    "        for column_name in columns_to_clean:\n",
    "            if column_name not in batch_df.columns:\n",
    "                print(f\"  - WARNING: Column '{column_name}' not found in this batch. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  - De-identifying column: '{column_name}' using {IMPLEMENTATION_APPROACH} approach\")\n",
    "            \n",
    "            total_in_batch = len(batch_df)\n",
    "            \n",
    "            # Prepare data for processing: (row_index, text_content)\n",
    "            row_data = [(idx, str(row[column_name])) for idx, row in batch_df.iterrows()]\n",
    "            \n",
    "            # Initialize results dictionary to maintain order\n",
    "            processed_results = {}\n",
    "            completed_count = 0\n",
    "            \n",
    "            # Use ThreadPoolExecutor for parallel processing\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                # Submit all tasks\n",
    "                future_to_index = {executor.submit(process_row_parallel, data): data[0] for data in row_data}\n",
    "                \n",
    "                # Process completed tasks as they finish\n",
    "                for future in as_completed(future_to_index):\n",
    "                    original_index, processed_text = future.result()\n",
    "                    processed_results[original_index] = processed_text\n",
    "                    completed_count += 1\n",
    "                    \n",
    "                    # Print progress\n",
    "                    print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "            \n",
    "            print()  # Newline after the progress bar for a column is complete\n",
    "            \n",
    "            # Reconstruct the processed data in the original order\n",
    "            processed_data = [processed_results[idx] for idx, _ in batch_df.iterrows()]\n",
    "            \n",
    "            # Update the dataframe with processed data\n",
    "            if REPLACE_ORIGINAL_COLUMN:\n",
    "                batch_df[column_name] = processed_data\n",
    "            else:\n",
    "                batch_df[f\"{column_name}_deidentified\"] = processed_data\n",
    "        \n",
    "        # Save the processed batch to CSV\n",
    "        try:\n",
    "            batch_df.to_csv(output_path, index=False)\n",
    "            print(f\"  - Saved batch to: '{output_path}'\")\n",
    "            processed_batches.append(batch_num)\n",
    "        except Exception as e:\n",
    "            print(f\"  - ERROR: Could not save batch {batch_num}: {e}\")\n",
    "    \n",
    "    print(f\"\\n--- Processing Complete ---\")\n",
    "    print(f\"All batches have been processed and saved with prefix '{output_prefix}'.\")\n",
    "    \n",
    "    # Print summary of processed and skipped batches\n",
    "    if processed_batches:\n",
    "        print(f\"\\nProcessed batches: {', '.join(map(str, processed_batches))}\")\n",
    "    if skipped_batches:\n",
    "        print(f\"Skipped batches (already existed): {', '.join(map(str, skipped_batches))}\")\n",
    "    \n",
    "    # Print the location of the output files\n",
    "    print(f\"\\nOutput files are located in: {output_dir}\")\n",
    "    print(f\"File naming pattern: {output_prefix}_part_X.csv where X is the batch number\")\n",
    "    \n",
    "    # List the output files that exist\n",
    "    existing_output_files = [f for f in os.listdir(output_dir) if f.startswith(output_prefix) and f.endswith('.csv')]\n",
    "    if existing_output_files:\n",
    "        print(f\"\\nFound {len(existing_output_files)} output files\")\n",
    "        for file in sorted(existing_output_files):\n",
    "            file_path = os.path.join(output_dir, file)\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
    "            print(f\"  - {file} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"\\nNo output files found with prefix '{output_prefix}' in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Process\n",
    "\n",
    "Execute the main function. This will start the process using the file and columns you specified in the Configuration section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and preparing CSV from 'tester.csv'...\n",
      "Total rows: 8. This will be processed in 1 batch(es).\n",
      "Output files will be saved in: /Users/david/Documents/Documents - David’s iMac/GitHub/deid-ollama\n",
      "Each text will undergo 2 pass(es) through the LLM.\n",
      "Using implementation approach: semaphore\n",
      "Maximum concurrent requests to Ollama: 3\n",
      "\n",
      "--- Processing Batch 1/1 ---\n",
      "  - De-identifying column: 'patient_id' using semaphore approach\n",
      "    - Row 8/8Pass 2/2\n",
      "  - De-identifying column: 'first_name' using semaphore approach\n",
      "    - Row 8/8Pass 2/2\n",
      "  - De-identifying column: 'last_name' using semaphore approach\n",
      "    - Row 8/8Pass 2/2\n",
      "  - De-identifying column: 'dob' using semaphore approach\n",
      "    - Row 8/8Pass 2/2\n",
      "  - De-identifying column: 'phone_number' using semaphore approach\n",
      "    - Row 8/8Pass 2/2\n",
      "  - De-identifying column: 'note_text' using semaphore approach\n",
      "    - Row 8/8Pass 2/2\n",
      "  - Saved batch to: '/Users/david/Documents/Documents - David’s iMac/GitHub/deid-ollama/deidentified_output_post_LLM_part_1.csv'\n",
      "\n",
      "--- Processing Complete ---\n",
      "All batches have been processed and saved with prefix 'deidentified_output_post_LLM'.\n",
      "\n",
      "Processed batches: 1\n",
      "\n",
      "Output files are located in: /Users/david/Documents/Documents - David’s iMac/GitHub/deid-ollama\n",
      "File naming pattern: deidentified_output_post_LLM_part_X.csv where X is the batch number\n",
      "\n",
      "Found 1 output files\n",
      "  - deidentified_output_post_LLM_part_1.csv (0.00 MB)\n"
     ]
    }
   ],
   "source": [
    "# This cell runs the main function with the current settings.\n",
    "process_large_csv_complete(\n",
    "    input_path=INPUT_CSV_PATH,\n",
    "    output_prefix=OUTPUT_PREFIX, \n",
    "    columns_to_clean=COLUMNS_TO_CLEAN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
