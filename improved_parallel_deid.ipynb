{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV De-identification with Local LLM (Ollama)\n",
    "\n",
    "A robust tool for de-identifying Protected Health Information (PHI) in CSV files using a locally running Large Language Model (LLM) via Ollama. This notebook provides advanced concurrency control and multiple processing approaches to handle parallel processing efficiently while avoiding API overload issues.\n",
    "\n",
    "**⚠️ IMPORTANT**: This is **NOT validated** for production use. Only use for exploration and understanding.\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "### Core Functionality\n",
    "- **Multi-Column Processing**: De-identify one or more text columns in a single run\n",
    "- **Large File Support**: Automatically splits large CSVs into manageable batches\n",
    "- **Live Progress Tracking**: See which row is being processed in real-time\n",
    "- **Resumable Processing**: Can be stopped and restarted, automatically skipping completed batches\n",
    "- **Error Handling**: Rows with errors are retried multiple times before being marked as \"unable to deidentify\"\n",
    "- **Multi-Pass Processing**: Each text undergoes multiple passes through the LLM to catch missed PHI\n",
    "- **Flexible Output**: Option to either replace the original columns or add new de-identified columns\n",
    "\n",
    "### Advanced Concurrency Control\n",
    "1. **Rate Limiting**: Controls how many requests are sent to Ollama at once\n",
    "2. **Semaphore-based Concurrency Control**: Limits the number of concurrent API calls\n",
    "3. **Exponential Backoff Strategy**: Implements exponential backoff with jitter for retries\n",
    "4. **Queue-based Processing**: Option for more controlled processing flow\n",
    "5. **Multiple Implementation Options**: Choose the approach that works best for your system\n",
    "\n",
    "## Prerequisites:\n",
    "\n",
    "1. **Ollama installed and running** with parallel processing configured\n",
    "2. **LLM model available** (default: `gemma3:4b`)\n",
    "3. **Python dependencies** installed (see requirements.txt)\n",
    "\n",
    "## Usage Instructions:\n",
    "\n",
    "1. Fill out the Configuration section below with your CSV file path and column names\n",
    "2. Run all cells in the notebook\n",
    "3. De-identified CSV files will be created in the same directory as your input file\n",
    "\n",
    "Choose one of the implementation approaches below based on your system's needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import threading\n",
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration (User Input Required)\n",
    "\n",
    "**Please fill in the variables in the cell below before running the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REQUIRED: Specify the path to your CSV file ---\n",
    "INPUT_CSV_PATH = \"tester.csv\" \n",
    "\n",
    "# --- REQUIRED: List the names of the columns you want to de-identify ---\n",
    "COLUMNS_TO_CLEAN = [\"patient_id\", \"first_name\", \"last_name\", \"dob\", \"phone_number\",\"note_text\"] \n",
    "\n",
    "# --- OPTIONAL: Advanced Settings ---\n",
    "REPLACE_ORIGINAL_COLUMN = True      # True: Replaces original column(s). False: Adds new de-identified column(s).\n",
    "OUTPUT_PREFIX = \"deidentified_output_post_LLM\" # The prefix for the output files\n",
    "MAX_ROWS_PER_BATCH = 100         # The max number of rows to process in a single batch file.\n",
    "\n",
    "# --- Concurrency Settings ---\n",
    "MAX_WORKERS = 5                   # Reduced from 10 to avoid overwhelming Ollama\n",
    "MAX_CONCURRENT_REQUESTS = 3       # Maximum number of concurrent requests to Ollama\n",
    "USE_RATE_LIMITING = True          # Enable rate limiting for API calls\n",
    "RATE_LIMIT_CALLS = 3              # Maximum calls per time period\n",
    "RATE_LIMIT_PERIOD = 1             # Time period in seconds\n",
    "\n",
    "# --- Processing Settings ---\n",
    "MAX_RETRIES = 4                    # Number of times to retry processing a row before marking it as \"unable to deidentify\"\n",
    "DEIDENTIFICATION_PASSES = 2        # Number of passes through the LLM to catch any PHI missed in the first pass\n",
    "IMPLEMENTATION_APPROACH = \"semaphore\"  # Options: \"semaphore\", \"rate_limit\", \"queue\", \"process_pool\"\n",
    "\n",
    "# --- Ollama Settings ---\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"gemma3:4b\"             # Or any other model you have available\n",
    "MAX_CHUNK_SIZE = 5000               # Max characters for a single note before it's split for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Functions with Concurrency Control\n",
    "\n",
    "These functions handle the communication with the Ollama API with robust concurrency control to prevent overwhelming Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a semaphore to limit concurrent API calls\n",
    "api_semaphore = threading.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# Create a rate limiter\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls, period):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = []\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            # Remove calls older than the period\n",
    "            self.calls = [t for t in self.calls if now - t < self.period]\n",
    "            \n",
    "            # If we've reached the maximum calls, wait until we can make another\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_time = self.period - (now - self.calls[0])\n",
    "                if sleep_time > 0:\n",
    "                    time.sleep(sleep_time)\n",
    "                    \n",
    "            # Add the current call time\n",
    "            self.calls.append(time.time())\n",
    "            \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "\n",
    "rate_limiter = RateLimiter(RATE_LIMIT_CALLS, RATE_LIMIT_PERIOD)\n",
    "\n",
    "# Queue for controlled processing\n",
    "request_queue = queue.Queue()\n",
    "response_queue = queue.Queue()\n",
    "\n",
    "def call_ollama_api_with_semaphore(text_chunk, row_index=None, retry_count=0):\n",
    "    \"\"\"\n",
    "    Sends a text chunk to the Ollama API with semaphore-based concurrency control.\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "    \n",
    "    prompt = create_prompt(text_chunk)\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    # Use the semaphore to limit concurrent requests\n",
    "    with api_semaphore:\n",
    "        try:\n",
    "            # Add jitter to avoid thundering herd problem\n",
    "            time.sleep(random.uniform(0.1, 0.5))\n",
    "            response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "            return response_data.get('response', '').strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retry_count += 1\n",
    "            if row_index is not None:\n",
    "                print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            if retry_count < MAX_RETRIES:\n",
    "                backoff_time = min(2 ** retry_count + random.uniform(0, 1), 10)\n",
    "                time.sleep(backoff_time)\n",
    "                return call_ollama_api_with_semaphore(text_chunk, row_index, retry_count)\n",
    "            else:\n",
    "                return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "\n",
    "def call_ollama_api_with_rate_limit(text_chunk, row_index=None, retry_count=0):\n",
    "    \"\"\"\n",
    "    Sends a text chunk to the Ollama API with rate limiting.\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "    \n",
    "    prompt = create_prompt(text_chunk)\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    # Use rate limiting\n",
    "    with rate_limiter:\n",
    "        try:\n",
    "            response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "            return response_data.get('response', '').strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retry_count += 1\n",
    "            if row_index is not None:\n",
    "                print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            if retry_count < MAX_RETRIES:\n",
    "                backoff_time = min(2 ** retry_count + random.uniform(0, 1), 10)\n",
    "                time.sleep(backoff_time)\n",
    "                return call_ollama_api_with_rate_limit(text_chunk, row_index, retry_count)\n",
    "            else:\n",
    "                return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "\n",
    "def queue_worker():\n",
    "    \"\"\"\n",
    "    Worker function for queue-based processing.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # Get a task from the queue\n",
    "            task = request_queue.get(timeout=1)\n",
    "            if task is None:  # Sentinel value to stop the worker\n",
    "                request_queue.task_done()\n",
    "                break\n",
    "                \n",
    "            text_chunk, row_index, retry_count = task\n",
    "            \n",
    "            # Process the task\n",
    "            try:\n",
    "                prompt = create_prompt(text_chunk)\n",
    "                payload = {\n",
    "                    \"model\": MODEL_NAME,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False\n",
    "                }\n",
    "                \n",
    "                # Add jitter to avoid thundering herd problem\n",
    "                time.sleep(random.uniform(0.1, 0.5))\n",
    "                response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "                response.raise_for_status()\n",
    "                response_data = response.json()\n",
    "                result = response_data.get('response', '').strip()\n",
    "                \n",
    "                # Put the result in the response queue\n",
    "                response_queue.put((row_index, result))\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                retry_count += 1\n",
    "                if row_index is not None:\n",
    "                    print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "                \n",
    "                # Retry if we haven't reached the maximum retries\n",
    "                if retry_count < MAX_RETRIES:\n",
    "                    # Exponential backoff with jitter\n",
    "                    backoff_time = min(2 ** retry_count + random.uniform(0, 1), 10)\n",
    "                    time.sleep(backoff_time)\n",
    "                    request_queue.put((text_chunk, row_index, retry_count))\n",
    "                else:\n",
    "                    response_queue.put((row_index, \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"))\n",
    "            \n",
    "            # Mark the task as done\n",
    "            request_queue.task_done()\n",
    "                \n",
    "        except queue.Empty:\n",
    "            # If the queue is empty, wait a bit and try again\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in queue worker: {e}\")\n",
    "            request_queue.task_done()\n",
    "\n",
    "def create_prompt(text_chunk):\n",
    "    \"\"\"\n",
    "    Creates the prompt for the Ollama API.\n",
    "    \"\"\"\n",
    "    return f\"\"\"[SYSTEM]\n",
    "    You are an automated de-identification system. Your sole function is to process the text provided and return a clean version with all Protected Health Information (PHI) replaced by the appropriate category label. You are precise and do not deviate from your instructions.\n",
    "\n",
    "    **Instructions:**\n",
    "\n",
    "    1.  **Identify and Replace:** Your task is to find all instances of the following PHI categories in the user-provided text and replace them with their corresponding labels:\n",
    "        * Names of individuals: `[PERSON]`\n",
    "        * All dates (including full dates, partial dates, and days of the week): `[DATE]`\n",
    "        * Geographical locations (cities, states, addresses, etc.): `[LOCATION]`\n",
    "        * Phone and fax numbers: `[PHONE]`\n",
    "        * Email addresses: `[EMAIL]`\n",
    "        * Any identification numbers (e.g., Social Security Numbers, Medical Record Numbers, account numbers): `[ID_NUMBER]`\n",
    "        * Other categories such as medical conditions, medications, and health-related information should remain unchanged unless they contain PHI.\n",
    "\n",
    "    2.  **Output Format:**\n",
    "        * The output MUST be only the modified text.\n",
    "        * Do NOT include any introductory phrases, explanations, or apologies.\n",
    "        * The response should not contain any of the original PHI.\n",
    "\n",
    "    **Examples:**\n",
    "\n",
    "    * **Input:** \"John Smith visited the clinic on January 5, 2024. His MRN is 12345. He has HTN and takes lisinopril.\"\n",
    "    * **Output:** \"[PERSON] visited the clinic on [DATE]. His MRN is [ID_NUMBER]. He has HTN and takes lisinopril.\"\n",
    "\n",
    "    * **Input:** \"Patient can be reached at (555) 123-4567 or jane.doe@email.com.\"\n",
    "    * **Output:** \"Patient can be reached at [PHONE] or [EMAIL].\"\n",
    "\n",
    "    **Text for De-identification:**\n",
    "\n",
    "    ---\n",
    "    {text_chunk}\n",
    "    ---\n",
    "    \"\"\"\n",
    "\n",
    "def deidentify_text(full_text, row_index=None, implementation=IMPLEMENTATION_APPROACH):\n",
    "    \"\"\"\n",
    "    Manages the de-identification of a full text string, handling chunking if necessary.\n",
    "    Performs multiple passes through the LLM based on DEIDENTIFICATION_PASSES setting.\n",
    "    Uses the specified implementation approach for API calls.\n",
    "    \"\"\"\n",
    "    if not isinstance(full_text, str) or not full_text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Select the appropriate API call function based on the implementation approach\n",
    "    if implementation == \"semaphore\":\n",
    "        api_call_func = call_ollama_api_with_semaphore\n",
    "    elif implementation == \"rate_limit\":\n",
    "        api_call_func = call_ollama_api_with_rate_limit\n",
    "    else:\n",
    "        # Default to semaphore-based approach\n",
    "        api_call_func = call_ollama_api_with_semaphore\n",
    "    \n",
    "    # Process the text through multiple passes to catch any missed PHI\n",
    "    processed_text = full_text\n",
    "    \n",
    "    for pass_num in range(DEIDENTIFICATION_PASSES):\n",
    "        if pass_num > 0 and row_index is not None:\n",
    "            print(f\"\\r    - Row {row_index}: Pass {pass_num+1}/{DEIDENTIFICATION_PASSES}\", end=\"\")\n",
    "            \n",
    "        if len(processed_text) <= MAX_CHUNK_SIZE:\n",
    "            processed_text = api_call_func(processed_text, row_index)\n",
    "        else:\n",
    "            chunks = [processed_text[i:i + MAX_CHUNK_SIZE] for i in range(0, len(processed_text), MAX_CHUNK_SIZE)]\n",
    "            processed_chunks = []\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                processed_chunk = api_call_func(chunk, row_index)\n",
    "                processed_chunks.append(processed_chunk)\n",
    "                \n",
    "            processed_text = \"\".join(processed_chunks)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def process_row_parallel(args):\n",
    "    \"\"\"\n",
    "    Helper function for parallel processing of individual rows.\n",
    "    Returns tuple of (original_index, processed_text) to maintain order.\n",
    "    \"\"\"\n",
    "    row_index, original_text = args\n",
    "    processed_text = deidentify_text(original_text, row_index, IMPLEMENTATION_APPROACH)\n",
    "    return (row_index, processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Queue-based Processing Implementation\n",
    "\n",
    "This is an alternative implementation using a queue-based approach for more controlled processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_queue(row_data, total_in_batch):\n",
    "    \"\"\"\n",
    "    Process rows using a queue-based approach for better control over concurrency.\n",
    "    \"\"\"\n",
    "    # Clear the queues\n",
    "    while not request_queue.empty():\n",
    "        try:\n",
    "            request_queue.get_nowait()\n",
    "            request_queue.task_done()\n",
    "        except queue.Empty:\n",
    "            break\n",
    "            \n",
    "    while not response_queue.empty():\n",
    "        try:\n",
    "            response_queue.get_nowait()\n",
    "        except queue.Empty:\n",
    "            break\n",
    "    \n",
    "    # Start worker threads\n",
    "    workers = []\n",
    "    for _ in range(MAX_CONCURRENT_REQUESTS):\n",
    "        worker = threading.Thread(target=queue_worker)\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "        workers.append(worker)\n",
    "    \n",
    "    # Add tasks to the queue\n",
    "    for row_index, original_text in row_data:\n",
    "        request_queue.put((original_text, row_index, 0))\n",
    "    \n",
    "    # Wait for all tasks to be processed\n",
    "    processed_results = {}\n",
    "    completed_count = 0\n",
    "    \n",
    "    while completed_count < len(row_data):\n",
    "        try:\n",
    "            row_index, processed_text = response_queue.get(timeout=1)\n",
    "            processed_results[row_index] = processed_text\n",
    "            completed_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "            \n",
    "        except queue.Empty:\n",
    "            # If the queue is empty, wait a bit and try again\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    # Stop the workers\n",
    "    for _ in range(len(workers)):\n",
    "        request_queue.put(None)  # Sentinel value to stop the worker\n",
    "    \n",
    "    # Wait for all workers to finish\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "    \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Pool Implementation\n",
    "\n",
    "This implementation uses ProcessPoolExecutor instead of ThreadPoolExecutor for potentially better performance on CPU-bound tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_process_pool(row_data, total_in_batch):\n",
    "    \"\"\"\n",
    "    Process rows using ProcessPoolExecutor for potentially better performance on CPU-bound tasks.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary to maintain order\n",
    "    processed_results = {}\n",
    "    completed_count = 0\n",
    "    \n",
    "    # Define a worker function that can be pickled\n",
    "    def worker_func(row_index, text):\n",
    "        return row_index, deidentify_text(text, row_index, \"semaphore\")\n",
    "    \n",
    "    # Use ProcessPoolExecutor for parallel processing\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_index = {executor.submit(worker_func, idx, text): idx for idx, text in row_data}\n",
    "        \n",
    "        # Process completed tasks as they finish\n",
    "        for future in as_completed(future_to_index):\n",
    "            original_index, processed_text = future.result()\n",
    "            processed_results[original_index] = processed_text\n",
    "            completed_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "    \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Processing Function\n",
    "\n",
    "This is the complete main processing function that handles CSV reading, batching, and de-identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_csv_complete(input_path, output_prefix, columns_to_clean):\n",
    "    \"\"\"\n",
    "    Reads a potentially large CSV, splits it into batches, de-identifies the \n",
    "    specified columns, and saves new numbered CSV files for each batch.\n",
    "    Uses the specified implementation approach for concurrency control.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_path) or input_path == \"/path/to/your/file.csv\":\n",
    "        print(\"ERROR: Input file not found or path not set.\")\n",
    "        print(f\"Please update the 'INPUT_CSV_PATH' variable in the Configuration section.\")\n",
    "        return\n",
    "    \n",
    "    # Get the directory of the input file to save output files in the same location\n",
    "    output_dir = os.path.dirname(os.path.abspath(input_path))\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading and preparing CSV from '{input_path}'...\")\n",
    "        df_iterator = pd.read_csv(input_path, chunksize=MAX_ROWS_PER_BATCH, on_bad_lines='warn')\n",
    "        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            total_rows = sum(1 for row in f) - 1 # -1 for header\n",
    "        num_batches = math.ceil(total_rows / MAX_ROWS_PER_BATCH)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading input file: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total rows: {total_rows}. This will be processed in {num_batches} batch(es).\")\n",
    "    print(f\"Output files will be saved in: {output_dir}\")\n",
    "    print(f\"Each text will undergo {DEIDENTIFICATION_PASSES} pass(es) through the LLM.\")\n",
    "    print(f\"Using implementation approach: {IMPLEMENTATION_APPROACH}\")\n",
    "    print(f\"Maximum concurrent requests to Ollama: {MAX_CONCURRENT_REQUESTS}\")\n",
    "    \n",
    "    # Track processed batches for summary\n",
    "    processed_batches = []\n",
    "    skipped_batches = []\n",
    "\n",
    "    for i, batch_df in enumerate(df_iterator):\n",
    "        batch_num = i + 1\n",
    "        output_path = os.path.join(output_dir, f\"{output_prefix}_part_{batch_num}.csv\")\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"\\nOutput file '{output_path}' already exists. Skipping Batch {batch_num}.\")\n",
    "            skipped_batches.append(batch_num)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Processing Batch {batch_num}/{num_batches} ---\")\n",
    "        \n",
    "        for column_name in columns_to_clean:\n",
    "            if column_name not in batch_df.columns:\n",
    "                print(f\"  - WARNING: Column '{column_name}' not found in this batch. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  - De-identifying column: '{column_name}' using {IMPLEMENTATION_APPROACH} approach\")\n",
    "            \n",
    "            total_in_batch = len(batch_df)\n",
    "            \n",
    "            # Prepare data for processing: (row_index, text_content)\n",
    "            row_data = [(idx, row[column_name]) for idx, row in batch_df.iterrows()]\n",
    "            \n",
    "            # Choose the appropriate processing method based on the implementation approach\n",
    "            if IMPLEMENTATION_APPROACH == \"queue\":\n",
    "                processed_results = process_with_queue(row_data, total_in_batch)\n",
    "            elif IMPLEMENTATION_APPROACH == \"process_pool\":\n",
    "                processed_results = process_with_process_pool(row_data, total_in_batch)\n",
    "            else:  # Default to ThreadPoolExecutor with semaphore or rate limiting\n",
    "                # Initialize results dictionary to maintain order\n",
    "                processed_results = {}\n",
    "                completed_count = 0\n",
    "                \n",
    "                # Use ThreadPoolExecutor for parallel processing\n",
    "                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                    # Submit all tasks\n",
    "                    future_to_index = {executor.submit(process_row_parallel, data): data[0] for data in row_data}\n",
    "                    \n",
    "                    # Process completed tasks as they finish\n",
    "                    for future in as_completed(future_to_index):\n",
    "                        original_index, processed_text = future.result()\n",
    "                        processed_results[original_index] = processed_text\n",
    "                        completed_count += 1\n",
    "                        \n",
    "                        # Print progress\n",
    "                        print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "            \n",
    "            print()  # Newline after the progress bar for a column is complete\n",
    "            \n",
    "            # Reconstruct the processed data in the original order\n",
    "            processed_data = [processed_results[idx] for idx, _ in batch_df.iterrows()]\n",
    "            \n",
    "            # Update the dataframe with processed data\n",
    "            if REPLACE_ORIGINAL_COLUMN:\n",
    "                batch_df[column_name] = processed_data\n",
    "            else:\n",
    "                batch_df[f\"{column_name}_deidentified\"] = processed_data\n",
    "        \n",
    "        # Save the processed batch to CSV\n",
    "        try:\n",
    "            batch_df.to_csv(output_path, index=False)\n",
    "            print(f\"  - Saved batch to: '{output_path}'\")\n",
    "            processed_batches.append(batch_num)\n",
    "        except Exception as e:\n",
    "            print(f\"  - ERROR: Could not save batch {batch_num}: {e}\")\n",
    "    \n",
    "    print(f\"\\n--- Processing Complete ---\")\n",
    "    print(f\"All batches have been processed and saved with prefix '{output_prefix}'.\")\n",
    "    \n",
    "    # Print summary of processed and skipped batches\n",
    "    if processed_batches:\n",
    "        print(f\"\\nProcessed batches: {', '.join(map(str, processed_batches))}\")\n",
    "    if skipped_batches:\n",
    "        print(f\"Skipped batches (already existed): {', '.join(map(str, skipped_batches))}\")\n",
    "    \n",
    "    # Print the location of the output files\n",
    "    print(f\"\\nOutput files are located in: {output_dir}\")\n",
    "    print(f\"File naming pattern: {output_prefix}_part_X.csv where X is the batch number\")\n",
    "    \n",
    "    # List the output files that exist\n",
    "    existing_output_files = [f for f in os.listdir(output_dir) if f.startswith(output_prefix) and f.endswith('.csv')]\n",
    "    if existing_output_files:\n",
    "        print(f\"\\nFound {len(existing_output_files)} output files:\")\n",
    "        for file in sorted(existing_output_files):\n",
    "            file_path = os.path.join(output_dir, file)\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
    "            print(f\"  - {file} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"\\nNo output files found with prefix '{output_prefix}' in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Process\n",
    "\n",
    "Execute the main function. This will start the process using the file and columns you specified in the Configuration section. If you run this cell a second time, it will find the generated output files and skip the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and preparing CSV from 'tester.csv'...\n",
      "Total rows: 5. This will be processed in 1 batch(es).\n",
      "Output files will be saved in: /Users/david/Documents/Documents - David’s iMac/GitHub/deid-ollama\n",
      "Each text will undergo 2 pass(es) through the LLM.\n",
      "Using implementation approach: semaphore\n",
      "Maximum concurrent requests to Ollama: 3\n",
      "\n",
      "--- Processing Batch 1/1 ---\n",
      "  - De-identifying column: 'patient_id' using semaphore approach\n",
      "    - Row 5/5\n",
      "  - De-identifying column: 'first_name' using semaphore approach\n",
      "    - Row 5/5Pass 2/2\n",
      "  - De-identifying column: 'last_name' using semaphore approach\n",
      "    - Row 5/5Pass 2/2\n",
      "  - De-identifying column: 'dob' using semaphore approach\n",
      "    - Row 5/5Pass 2/2\n",
      "  - De-identifying column: 'phone_number' using semaphore approach\n",
      "    - Row 5/5Pass 2/2\n",
      "  - De-identifying column: 'note_text' using semaphore approach\n",
      "    - Row 5/5Pass 2/2\n",
      "  - Saved batch to: '/Users/david/Documents/Documents - David’s iMac/GitHub/deid-ollama/deidentified_output_post_LLM_part_1.csv'\n",
      "\n",
      "--- Processing Complete ---\n",
      "All batches have been processed and saved with prefix 'deidentified_output_post_LLM'.\n",
      "\n",
      "Processed batches: 1\n",
      "\n",
      "Output files are located in: /Users/david/Documents/Documents - David’s iMac/GitHub/deid-ollama\n",
      "File naming pattern: deidentified_output_post_LLM_part_X.csv where X is the batch number\n",
      "\n",
      "Found 1 output files:\n",
      "  - deidentified_output_post_LLM_part_1.csv (0.00 MB)\n"
     ]
    }
   ],
   "source": [
    "# This cell runs the main function with the settings you provided in the first notebook.\n",
    "process_large_csv_complete(\n",
    "    input_path=INPUT_CSV_PATH,\n",
    "    output_prefix=OUTPUT_PREFIX, \n",
    "    columns_to_clean=COLUMNS_TO_CLEAN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting\n",
    "\n",
    "If you're still experiencing issues with Ollama and parallel processing, here are some additional approaches to try:\n",
    "\n",
    "1. **Further reduce concurrency**: Try setting `MAX_CONCURRENT_REQUESTS` to 1 or 2\n",
    "2. **Increase delay between requests**: Modify the rate limiter to add more delay\n",
    "3. **Use a different model**: Some models may handle concurrent requests better than others\n",
    "4. **Run Ollama with more resources**: If possible, allocate more CPU/memory to Ollama\n",
    "5. **Use a different implementation approach**: Try each of the approaches (semaphore, rate_limit, queue, process_pool) to see which works best for your system\n",
    "\n",
    "You can also try running Ollama with the `--parallel` flag if available in your version, which may improve handling of concurrent requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of modifying settings for troubleshooting\n",
    "def troubleshoot_with_minimal_concurrency():\n",
    "    global MAX_CONCURRENT_REQUESTS, RATE_LIMIT_CALLS, RATE_LIMIT_PERIOD\n",
    "    \n",
    "    # Save original values\n",
    "    original_max_concurrent = MAX_CONCURRENT_REQUESTS\n",
    "    original_rate_limit_calls = RATE_LIMIT_CALLS\n",
    "    original_rate_limit_period = RATE_LIMIT_PERIOD\n",
    "    \n",
    "    # Apply minimal concurrency settings\n",
    "    MAX_CONCURRENT_REQUESTS = 1\n",
    "    RATE_LIMIT_CALLS = 1\n",
    "    RATE_LIMIT_PERIOD = 2  # 1 call every 2 seconds\n",
    "    \n",
    "    # Recreate the semaphore and rate limiter with new settings\n",
    "    global api_semaphore, rate_limiter\n",
    "    api_semaphore = threading.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "    rate_limiter = RateLimiter(RATE_LIMIT_CALLS, RATE_LIMIT_PERIOD)\n",
    "    \n",
    "    print(f\"Applied minimal concurrency settings:\")\n",
    "    print(f\"  - MAX_CONCURRENT_REQUESTS: {MAX_CONCURRENT_REQUESTS} (was {original_max_concurrent})\")\n",
    "    print(f\"  - RATE_LIMIT_CALLS: {RATE_LIMIT_CALLS} (was {original_rate_limit_calls})\")\n",
    "    print(f\"  - RATE_LIMIT_PERIOD: {RATE_LIMIT_PERIOD} (was {original_rate_limit_period})\")\n",
    "    print(f\"\\nNow you can run the process_large_csv_complete function again with these settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you're still experiencing issues\n",
    "# troubleshoot_with_minimal_concurrency()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
