{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Parallel De-identification with Ollama\n",
    "\n",
    "This notebook provides several alternative approaches to handle parallel processing with Ollama while avoiding concurrent errors. It builds on the original de-identification notebook but adds more robust concurrency control mechanisms.\n",
    "\n",
    "## Key Improvements:\n",
    "\n",
    "1. **Rate Limiting**: Controls how many requests are sent to Ollama at once\n",
    "2. **Semaphore-based Concurrency Control**: Limits the number of concurrent API calls\n",
    "3. **Backoff Strategy**: Implements exponential backoff for retries\n",
    "4. **Queue-based Processing**: Option for a more controlled processing flow\n",
    "5. **Multiple Implementation Options**: Choose the approach that works best for your system\n",
    "\n",
    "Select one of the implementation approaches below based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import threading\n",
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration (User Input Required)\n",
    "\n",
    "**Please fill in the variables in the cell below before running the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REQUIRED: Specify the path to your CSV file ---\n",
    "INPUT_CSV_PATH = \"/Users/dli989/Documents/RECOVER-local/liebovitz-llm_deidentified_notes.csv\" \n",
    "\n",
    "# --- REQUIRED: List the names of the columns you want to de-identify ---\n",
    "COLUMNS_TO_CLEAN = [\"note_text\"] \n",
    "\n",
    "# --- OPTIONAL: Advanced Settings ---\n",
    "REPLACE_ORIGINAL_COLUMN = True      # True: Replaces original column(s). False: Adds new de-identified column(s).\n",
    "OUTPUT_PREFIX = \"deidentified_output_post_LLM\" # The prefix for the output files\n",
    "MAX_ROWS_PER_BATCH = 100         # The max number of rows to process in a single batch file.\n",
    "\n",
    "# --- Concurrency Settings ---\n",
    "MAX_WORKERS = 5                   # Reduced from 10 to avoid overwhelming Ollama\n",
    "MAX_CONCURRENT_REQUESTS = 3       # Maximum number of concurrent requests to Ollama\n",
    "USE_RATE_LIMITING = True          # Enable rate limiting for API calls\n",
    "RATE_LIMIT_CALLS = 3              # Maximum calls per time period\n",
    "RATE_LIMIT_PERIOD = 1             # Time period in seconds\n",
    "\n",
    "# --- Processing Settings ---\n",
    "MAX_RETRIES = 4                    # Number of times to retry processing a row before marking it as \"unable to deidentify\"\n",
    "DEIDENTIFICATION_PASSES = 2        # Number of passes through the LLM to catch any PHI missed in the first pass\n",
    "IMPLEMENTATION_APPROACH = \"semaphore\"  # Options: \"semaphore\", \"rate_limit\", \"queue\", \"process_pool\"\n",
    "\n",
    "# --- Ollama Settings ---\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"gemma3:4b\"             # Or any other model you have available\n",
    "MAX_CHUNK_SIZE = 5000               # Max characters for a single note before it's split for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Functions with Improved Concurrency Control\n",
    "\n",
    "These functions handle the communication with the Ollama API with better concurrency control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a semaphore to limit concurrent API calls\n",
    "api_semaphore = threading.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# Create a rate limiter\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls, period):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = []\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            # Remove calls older than the period\n",
    "            self.calls = [t for t in self.calls if now - t < self.period]\n",
    "            \n",
    "            # If we've reached the maximum calls, wait until we can make another\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_time = self.period - (now - self.calls[0])\n",
    "                if sleep_time > 0:\n",
    "                    time.sleep(sleep_time)\n",
    "                    \n",
    "            # Add the current call time\n",
    "            self.calls.append(time.time())\n",
    "            \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "\n",
    "rate_limiter = RateLimiter(RATE_LIMIT_CALLS, RATE_LIMIT_PERIOD)\n",
    "\n",
    "# Queue for controlled processing\n",
    "request_queue = queue.Queue()\n",
    "response_queue = queue.Queue()\n",
    "\n",
    "def call_ollama_api_with_semaphore(text_chunk, row_index=None, retry_count=0):\n",
    "    \"\"\"\n",
    "    Sends a text chunk to the Ollama API with semaphore-based concurrency control.\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "    \n",
    "    prompt = create_prompt(text_chunk)\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    # Use the semaphore to limit concurrent requests\n",
    "    with api_semaphore:\n",
    "        try:\n",
    "            # Add jitter to avoid thundering herd problem\n",
    "            time.sleep(random.uniform(0.1, 0.5))\n",
    "            response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "            return response_data.get('response', '').strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retry_count += 1\n",
    "            if row_index is not None:\n",
    "                print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            if retry_count < MAX_RETRIES:\n",
    "                backoff_time = min(2 ** retry_count + random.uniform(0, 1), 10)\n",
    "                time.sleep(backoff_time)\n",
    "                return call_ollama_api_with_semaphore(text_chunk, row_index, retry_count)\n",
    "            else:\n",
    "                return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "\n",
    "def call_ollama_api_with_rate_limit(text_chunk, row_index=None, retry_count=0):\n",
    "    \"\"\"\n",
    "    Sends a text chunk to the Ollama API with rate limiting.\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "    \n",
    "    prompt = create_prompt(text_chunk)\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    # Use rate limiting\n",
    "    with rate_limiter:\n",
    "        try:\n",
    "            response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "            return response_data.get('response', '').strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retry_count += 1\n",
    "            if row_index is not None:\n",
    "                print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            if retry_count < MAX_RETRIES:\n",
    "                backoff_time = min(2 ** retry_count + random.uniform(0, 1), 10)\n",
    "                time.sleep(backoff_time)\n",
    "                return call_ollama_api_with_rate_limit(text_chunk, row_index, retry_count)\n",
    "            else:\n",
    "                return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "\n",
    "def queue_worker():\n",
    "    \"\"\"\n",
    "    Worker function for queue-based processing.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # Get a task from the queue\n",
    "            task = request_queue.get(timeout=1)\n",
    "            if task is None:  # Sentinel value to stop the worker\n",
    "                request_queue.task_done()\n",
    "                break\n",
    "                \n",
    "            text_chunk, row_index, retry_count = task\n",
    "            \n",
    "            # Process the task\n",
    "            try:\n",
    "                prompt = create_prompt(text_chunk)\n",
    "                payload = {\n",
    "                    \"model\": MODEL_NAME,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False\n",
    "                }\n",
    "                \n",
    "                # Add jitter to avoid thundering herd problem\n",
    "                time.sleep(random.uniform(0.1, 0.5))\n",
    "                response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "                response.raise_for_status()\n",
    "                response_data = response.json()\n",
    "                result = response_data.get('response', '').strip()\n",
    "                \n",
    "                # Put the result in the response queue\n",
    "                response_queue.put((row_index, result))\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                retry_count += 1\n",
    "                if row_index is not None:\n",
    "                    print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "                \n",
    "                # Retry if we haven't reached the maximum retries\n",
    "                if retry_count < MAX_RETRIES:\n",
    "                    # Exponential backoff with jitter\n",
    "                    backoff_time = min(2 ** retry_count + random.uniform(0, 1), 10)\n",
    "                    time.sleep(backoff_time)\n",
    "                    request_queue.put((text_chunk, row_index, retry_count))\n",
    "                else:\n",
    "                    response_queue.put((row_index, \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"))\n",
    "            \n",
    "            # Mark the task as done\n",
    "            request_queue.task_done()\n",
    "                \n",
    "        except queue.Empty:\n",
    "            # If the queue is empty, wait a bit and try again\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in queue worker: {e}\")\n",
    "            request_queue.task_done()\n",
    "\n",
    "def create_prompt(text_chunk):\n",
    "    \"\"\"\n",
    "    Creates the prompt for the Ollama API.\n",
    "    \"\"\"\n",
    "    return f\"\"\"[SYSTEM]\n",
    "    You are an automated de-identification system. Your sole function is to process the text provided and return a clean version with all Protected Health Information (PHI) replaced by the appropriate category label. You are precise and do not deviate from your instructions.\n",
    "\n",
    "    **Instructions:**\n",
    "\n",
    "    1.  **Identify and Replace:** Your task is to find all instances of the following PHI categories in the user-provided text and replace them with their corresponding labels:\n",
    "        * Names of individuals: `[PERSON]`\n",
    "        * All dates (including full dates, partial dates, and days of the week): `[DATE]`\n",
    "        * Geographical locations (cities, states, addresses, etc.): `[LOCATION]`\n",
    "        * Phone and fax numbers: `[PHONE]`\n",
    "        * Email addresses: `[EMAIL]`\n",
    "        * Any identification numbers (e.g., Social Security Numbers, Medical Record Numbers, account numbers): `[ID_NUMBER]`\n",
    "\n",
    "    2.  **Output Format:**\n",
    "        * The output MUST be only the modified text.\n",
    "        * Do NOT include any introductory phrases, explanations, or apologies.\n",
    "        * The response should not contain any of the original PHI.\n",
    "\n",
    "    **Examples:**\n",
    "\n",
    "    * **Input:** \"John Smith visited the clinic on January 5, 2024. His MRN is 12345.\"\n",
    "    * **Output:** \"[PERSON] visited the clinic on [DATE]. His MRN is [ID_NUMBER].\"\n",
    "\n",
    "    * **Input:** \"Patient can be reached at (555) 123-4567 or jane.doe@email.com.\"\n",
    "    * **Output:** \"Patient can be reached at [PHONE] or [EMAIL].\"\n",
    "\n",
    "    **Text for De-identification:**\n",
    "\n",
    "    ---\n",
    "    {text_chunk}\n",
    "    ---\n",
    "    \"\"\"\n",
    "\n",
    "def deidentify_text(full_text, row_index=None, implementation=IMPLEMENTATION_APPROACH):\n",
    "    \"\"\"\n",
    "    Manages the de-identification of a full text string, handling chunking if necessary.\n",
    "    Performs multiple passes through the LLM based on DEIDENTIFICATION_PASSES setting.\n",
    "    Uses the specified implementation approach for API calls.\n",
    "    \"\"\"\n",
    "    if not isinstance(full_text, str) or not full_text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Select the appropriate API call function based on the implementation approach\n",
    "    if implementation == \"semaphore\":\n",
    "        api_call_func = call_ollama_api_with_semaphore\n",
    "    elif implementation == \"rate_limit\":\n",
    "        api_call_func = call_ollama_api_with_rate_limit\n",
    "    else:\n",
    "        # Default to semaphore-based approach\n",
    "        api_call_func = call_ollama_api_with_semaphore\n",
    "    \n",
    "    # Process the text through multiple passes to catch any missed PHI\n",
    "    processed_text = full_text\n",
    "    \n",
    "    for pass_num in range(DEIDENTIFICATION_PASSES):\n",
    "        if pass_num > 0 and row_index is not None:\n",
    "            print(f\"\\r    - Row {row_index}: Pass {pass_num+1}/{DEIDENTIFICATION_PASSES}\", end=\"\")\n",
    "            \n",
    "        if len(processed_text) <= MAX_CHUNK_SIZE:\n",
    "            processed_text = api_call_func(processed_text, row_index)\n",
    "        else:\n",
    "            chunks = [processed_text[i:i + MAX_CHUNK_SIZE] for i in range(0, len(processed_text), MAX_CHUNK_SIZE)]\n",
    "            processed_chunks = []\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                processed_chunk = api_call_func(chunk, row_index)\n",
    "                processed_chunks.append(processed_chunk)\n",
    "                \n",
    "            processed_text = \"\".join(processed_chunks)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def process_row_parallel(args):\n",
    "    \"\"\"\n",
    "    Helper function for parallel processing of individual rows.\n",
    "    Returns tuple of (original_index, processed_text) to maintain order.\n",
    "    \"\"\"\n",
    "    row_index, original_text = args\n",
    "    processed_text = deidentify_text(original_text, row_index, IMPLEMENTATION_APPROACH)\n",
    "    return (row_index, processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Queue-based Processing Implementation\n",
    "\n",
    "This is an alternative implementation using a queue-based approach for more controlled processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_queue(row_data, total_in_batch):\n",
    "    \"\"\"\n",
    "    Process rows using a queue-based approach for better control over concurrency.\n",
    "    \"\"\"\n",
    "    # Clear the queues\n",
    "    while not request_queue.empty():\n",
    "        try:\n",
    "            request_queue.get_nowait()\n",
    "            request_queue.task_done()\n",
    "        except queue.Empty:\n",
    "            break\n",
    "            \n",
    "    while not response_queue.empty():\n",
    "        try:\n",
    "            response_queue.get_nowait()\n",
    "        except queue.Empty:\n",
    "            break\n",
    "    \n",
    "    # Start worker threads\n",
    "    workers = []\n",
    "    for _ in range(MAX_CONCURRENT_REQUESTS):\n",
    "        worker = threading.Thread(target=queue_worker)\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "        workers.append(worker)\n",
    "    \n",
    "    # Add tasks to the queue\n",
    "    for row_index, original_text in row_data:\n",
    "        request_queue.put((original_text, row_index, 0))\n",
    "    \n",
    "    # Wait for all tasks to be processed\n",
    "    processed_results = {}\n",
    "    completed_count = 0\n",
    "    \n",
    "    while completed_count < len(row_data):\n",
    "        try:\n",
    "            row_index, processed_text = response_queue.get(timeout=1)\n",
    "            processed_results[row_index] = processed_text\n",
    "            completed_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "            \n",
    "        except queue.Empty:\n",
    "            # If the queue is empty, wait a bit and try again\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    # Stop the workers\n",
    "    for _ in range(len(workers)):\n",
    "        request_queue.put(None)  # Sentinel value to stop the worker\n",
    "    \n",
    "    # Wait for all workers to finish\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "    \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Pool Implementation\n",
    "\n",
    "This implementation uses ProcessPoolExecutor instead of ThreadPoolExecutor for potentially better performance on CPU-bound tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_process_pool(row_data, total_in_batch):\n",
    "    \"\"\"\n",
    "    Process rows using ProcessPoolExecutor for potentially better performance on CPU-bound tasks.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary to maintain order\n",
    "    processed_results = {}\n",
    "    completed_count = 0\n",
    "    \n",
    "    # Define a worker function that can be pickled\n",
    "    def worker_func(row_index, text):\n",
    "        return row_index, deidentify_text(text, row_index, \"semaphore\")\n",
    "    \n",
    "    # Use ProcessPoolExecutor for parallel processing\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_index = {executor.submit(worker_func, idx, text): idx for idx, text in row_data}\n",
    "        \n",
    "        # Process completed tasks as they finish\n",
    "        for future in as_completed(future_to_index):\n",
    "            original_index, processed_text = future.result()\n",
    "            processed_results[original_index] = processed_text\n",
    "            completed_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "    \n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Processing Function Stub\n",
    "\n",
    "This is a stub for the main processing function. The complete implementation is in the second notebook (`improved_parallel_deid_part2.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_csv(input_path, output_prefix, columns_to_clean):\n",
    "    \"\"\"\n",
    "    Stub for the main processing function.\n",
    "    The complete implementation is in improved_parallel_deid_part2.ipynb.\n",
    "    \"\"\"\n",
    "    print(\"This is a stub. Please run the complete implementation in improved_parallel_deid_part2.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
