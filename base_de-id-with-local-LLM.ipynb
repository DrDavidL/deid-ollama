{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV De-identification using a Local LLM with Ollama\n",
    "\n",
    "This notebook processes a user-specified CSV file to de-identify text in one or more columns. It uses a locally running LLM, accessed via the Ollama API, to identify and replace Protected Health Information (PHI) with category placeholders (e.g., `[PERSON]`, `[DATE]`).\n",
    "\n",
    "### How to Use:\n",
    "1.  **Fill out the Configuration section below.** You must provide the path to your input CSV and a list of the column names you want to clean.\n",
    "2.  **Run all cells.** The script will process your file in batches and create new, de-identified CSV files in the same directory.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Live Progress Tracking**: See which row is being processed in real-time for each batch.\n",
    "- **User-Friendly Setup**: No need to create sample data; just point the script to your file.\n",
    "- **Multi-Column Processing**: De-identify one or more text columns in the same run.\n",
    "- **Flexible Output**: Option to either replace the PHI column (default) or add a new de-identified column.\n",
    "- **Large File Support (Batching)**: Automatically splits large CSVs into smaller, numbered batches.\n",
    "- **Resumability**: If the script is stopped, it can be rerun and will automatically skip any completed batches.\n",
    "- **Error Handling**: Rows with errors are retried up to 4 times before being marked as \"unable to deidentify\".\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "1.  **Ollama is installed and running**: You must have Ollama installed on your system.\n",
    "2.  **Gemma model is available**: You need to have pulled a model. You can do this by running `ollama pull gemma3:4b` in your terminal.\n",
    "3.  **Python Libraries**: Ensure you have `pandas` and `requests` installed (`pip install pandas requests`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration (User Input Required)\n",
    "**Please fill in the variables in the cell below before running the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REQUIRED: Specify the path to your CSV file ---\n",
    "INPUT_CSV_PATH = \"/Users/dli989/Documents/RECOVER-local/liebovitz-llm_deidentified_notes.csv\" # Example: \"C:/Users/YourUser/Desktop/my_notes.csv\"\n",
    "\n",
    "# --- REQUIRED: List the names of the columns you want to de-identify ---\n",
    "COLUMNS_TO_CLEAN = [\"note_text\"] # Example: [\"clinical_notes\", \"another_column_with_phi\"]\n",
    "\n",
    "# --- OPTIONAL: Advanced Settings ---\n",
    "REPLACE_ORIGINAL_COLUMN = True      # True: Replaces original column(s). False: Adds new de-identified column(s).\n",
    "OUTPUT_PREFIX = \"deidentified_output_post_LLM\" # The prefix for the output files (e.g., deidentified_output_part_1.csv)\n",
    "MAX_ROWS_PER_BATCH = 300         # The max number of rows to process in a single batch file.\n",
    "MAX_WORKERS = 10                   # Number of parallel threads for API calls (adjust based on your system)\n",
    "MAX_RETRIES = 4                    # Number of times to retry processing a row before marking it as \"unable to deidentify\"\n",
    "\n",
    "# --- OPTIONAL: Ollama Settings ---\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"gemma3:4b\"             # Or any other model you have available\n",
    "MAX_CHUNK_SIZE = 5000               # Max characters for a single note before it's split for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Functions\n",
    "\n",
    "These functions handle the communication with the Ollama API and the logic for chunking and reassembling the text for a single note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama_api(text_chunk, row_index=None, retry_count=0):\n",
    "    \"\"\"\n",
    "    Sends a text chunk to the Ollama API for de-identification.\n",
    "    Includes retry logic to attempt processing up to MAX_RETRIES times.\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a highly accurate de-identification tool. Your only task is to find and replace all Protected Health Information (PHI) in the following text with a category label in brackets. \n",
    "    Replace names with [PERSON], dates with [DATE], locations with [LOCATION], phone numbers with [PHONE], email addresses with [EMAIL], identification numbers (like SSN, MRN) with [ID_NUMBER].\n",
    "    ABSOLUTELY DO NOT include any commentary, greetings, or explanations in your response. Only return the modified text. Be careful to recognize if first name and last name are together, e.g,, \"jennifersmith\" which should be replaced with [PERSON].\n",
    "    \n",
    "    Here is the text:\n",
    "    ---\n",
    "    {text_chunk}\n",
    "    ---\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        response_data = response.json()\n",
    "        return response_data.get('response', '').strip()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # The leading \\n is removed to avoid breaking the progress bar line.\n",
    "        retry_count += 1\n",
    "        if row_index is not None:\n",
    "            print(f\"\\nError processing row {row_index}: {e} (Attempt {retry_count}/{MAX_RETRIES})\")\n",
    "        \n",
    "        # Wait briefly before retrying to avoid overwhelming the API\n",
    "        if retry_count < MAX_RETRIES:\n",
    "            time.sleep(2)  # Short delay before retry\n",
    "            return call_ollama_api(text_chunk, row_index, retry_count)\n",
    "        else:\n",
    "            return \"[UNABLE TO DEIDENTIFY: Maximum retry attempts reached]\"\n",
    "\n",
    "def deidentify_text(full_text, row_index=None):\n",
    "    \"\"\"\n",
    "    Manages the de-identification of a full text string, handling chunking if necessary.\n",
    "    \"\"\"\n",
    "    if not isinstance(full_text, str) or not full_text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    if len(full_text) <= MAX_CHUNK_SIZE:\n",
    "        return call_ollama_api(full_text, row_index)\n",
    "    \n",
    "    chunks = [full_text[i:i + MAX_CHUNK_SIZE] for i in range(0, len(full_text), MAX_CHUNK_SIZE)]\n",
    "    processed_chunks = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        processed_chunk = call_ollama_api(chunk, row_index)\n",
    "        processed_chunks.append(processed_chunk)\n",
    "        \n",
    "    return \"\".join(processed_chunks)\n",
    "\n",
    "def process_row_parallel(args):\n",
    "    \"\"\"\n",
    "    Helper function for parallel processing of individual rows.\n",
    "    Returns tuple of (original_index, processed_text) to maintain order.\n",
    "    \"\"\"\n",
    "    row_index, original_text = args\n",
    "    processed_text = deidentify_text(original_text, row_index)\n",
    "    return (row_index, processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Processing and Batching Logic\n",
    "\n",
    "This section reads the CSV, splits it into batches, and processes each specified column row by row to provide a live progress update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_csv(input_path, output_prefix, columns_to_clean):\n",
    "    \"\"\"\n",
    "    Reads a potentially large CSV, splits it into batches, de-identifies the \n",
    "    specified columns, and saves new numbered CSV files for each batch.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_path) or input_path == \"/path/to/your/file.csv\":\n",
    "        print(\"ERROR: Input file not found or path not set.\")\n",
    "        print(f\"Please update the 'INPUT_CSV_PATH' variable in the Configuration section.\")\n",
    "        return\n",
    "    \n",
    "    # Get the directory of the input file to save output files in the same location\n",
    "    output_dir = os.path.dirname(os.path.abspath(input_path))\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading and preparing CSV from '{input_path}'...\")\n",
    "        df_iterator = pd.read_csv(input_path, chunksize=MAX_ROWS_PER_BATCH, on_bad_lines='warn')\n",
    "        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            total_rows = sum(1 for row in f) - 1 # -1 for header\n",
    "        num_batches = math.ceil(total_rows / MAX_ROWS_PER_BATCH)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading input file: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total rows: {total_rows}. This will be processed in {num_batches} batch(es).\")\n",
    "    print(f\"Output files will be saved in: {output_dir}\")\n",
    "    \n",
    "    # Track processed batches for summary\n",
    "    processed_batches = []\n",
    "    skipped_batches = []\n",
    "\n",
    "    for i, batch_df in enumerate(df_iterator):\n",
    "        batch_num = i + 1\n",
    "        output_path = os.path.join(output_dir, f\"{output_prefix}_part_{batch_num}.csv\")\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"\\nOutput file '{output_path}' already exists. Skipping Batch {batch_num}.\")\n",
    "            skipped_batches.append(batch_num)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Processing Batch {batch_num}/{num_batches} ---\")\n",
    "        \n",
    "        for column_name in columns_to_clean:\n",
    "            if column_name not in batch_df.columns:\n",
    "                print(f\"  - WARNING: Column '{column_name}' not found in this batch. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  - De-identifying column: '{column_name}' using {MAX_WORKERS} parallel workers\")\n",
    "            \n",
    "            total_in_batch = len(batch_df)\n",
    "            \n",
    "            # Prepare data for parallel processing: (row_index, text_content)\n",
    "            row_data = [(idx, row[column_name]) for idx, row in batch_df.iterrows()]\n",
    "            \n",
    "            # Initialize results dictionary to maintain order\n",
    "            processed_results = {}\n",
    "            completed_count = 0\n",
    "            \n",
    "            # Use ThreadPoolExecutor for parallel processing\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                # Submit all tasks\n",
    "                future_to_index = {executor.submit(process_row_parallel, data): data[0] for data in row_data}\n",
    "                \n",
    "                # Process completed tasks as they finish\n",
    "                for future in as_completed(future_to_index):\n",
    "                    original_index, processed_text = future.result()\n",
    "                    processed_results[original_index] = processed_text\n",
    "                    completed_count += 1\n",
    "                    \n",
    "                    # Print progress\n",
    "                    print(f\"\\r    - Row {completed_count}/{total_in_batch}\", end=\"\")\n",
    "            \n",
    "            print() # Newline after the progress bar for a column is complete\n",
    "            \n",
    "            # Reconstruct the processed data in the original order\n",
    "            processed_data = [processed_results[idx] for idx, _ in batch_df.iterrows()]\n",
    "            \n",
    "            # Update the dataframe with processed data\n",
    "            if REPLACE_ORIGINAL_COLUMN:\n",
    "                batch_df[column_name] = processed_data\n",
    "            else:\n",
    "                batch_df[f\"{column_name}_deidentified\"] = processed_data\n",
    "        \n",
    "        # Save the processed batch to CSV\n",
    "        try:\n",
    "            batch_df.to_csv(output_path, index=False)\n",
    "            print(f\"  - Saved batch to: '{output_path}'\")\n",
    "            processed_batches.append(batch_num)\n",
    "        except Exception as e:\n",
    "            print(f\"  - ERROR: Could not save batch {batch_num}: {e}\")\n",
    "    \n",
    "    print(f\"\\n--- Processing Complete ---\")\n",
    "    print(f\"All batches have been processed and saved with prefix '{output_prefix}'.\")\n",
    "    \n",
    "    # Print summary of processed and skipped batches\n",
    "    if processed_batches:\n",
    "        print(f\"\\nProcessed batches: {', '.join(map(str, processed_batches))}\")\n",
    "    if skipped_batches:\n",
    "        print(f\"Skipped batches (already existed): {', '.join(map(str, skipped_batches))}\")\n",
    "    \n",
    "    # Print the location of the output files\n",
    "    print(f\"\\nOutput files are located in: {output_dir}\")\n",
    "    print(f\"File naming pattern: {output_prefix}_part_X.csv where X is the batch number\")\n",
    "    \n",
    "    # List the output files that exist\n",
    "    existing_output_files = [f for f in os.listdir(output_dir) if f.startswith(output_prefix) and f.endswith('.csv')]\n",
    "    if existing_output_files:\n",
    "        print(f\"\\nFound {len(existing_output_files)} output files:\")\n",
    "        for file in sorted(existing_output_files):\n",
    "            file_path = os.path.join(output_dir, file)\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
    "            print(f\"  - {file} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"\\nNo output files found with prefix '{output_prefix}' in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Process\n",
    "\n",
    "Execute the main function. This will start the process using the file and columns you specified in the Configuration section. If you run this cell a second time, it will find the generated output files and skip the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and preparing CSV from '/Users/dli989/Documents/RECOVER-local/liebovitz-llm_deidentified_notes.csv'...\n",
      "Total rows: 604482. This will be processed in 2015 batch(es).\n",
      "Output files will be saved in: /Users/dli989/Documents/RECOVER-local\n",
      "\n",
      "Output file '/Users/dli989/Documents/RECOVER-local/deidentified_output_post_LLM_part_1.csv' already exists. Skipping Batch 1.\n",
      "\n",
      "Output file '/Users/dli989/Documents/RECOVER-local/deidentified_output_post_LLM_part_2.csv' already exists. Skipping Batch 2.\n",
      "\n",
      "Output file '/Users/dli989/Documents/RECOVER-local/deidentified_output_post_LLM_part_3.csv' already exists. Skipping Batch 3.\n",
      "\n",
      "Output file '/Users/dli989/Documents/RECOVER-local/deidentified_output_post_LLM_part_4.csv' already exists. Skipping Batch 4.\n",
      "\n",
      "Output file '/Users/dli989/Documents/RECOVER-local/deidentified_output_post_LLM_part_5.csv' already exists. Skipping Batch 5.\n",
      "\n",
      "Output file '/Users/dli989/Documents/RECOVER-local/deidentified_output_post_LLM_part_6.csv' already exists. Skipping Batch 6.\n",
      "\n",
      "--- Processing Batch 7/2015 ---\n",
      "  - De-identifying column: 'note_text' using 10 parallel workers\n"
     ]
    }
   ],
   "source": [
    "# This cell runs the main function with the settings you provided at the top.\n",
    "process_large_csv(\n",
    "    input_path=INPUT_CSV_PATH,\n",
    "    output_prefix=OUTPUT_PREFIX, \n",
    "    columns_to_clean=COLUMNS_TO_CLEAN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
